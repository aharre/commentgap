{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, root_mean_squared_log_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import root_mean_squared_log_error\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pinned_scaled = pd.read_csv('data/data_pin_scaled.csv', index_col=0)\n",
    "df_scaled = pd.read_csv('data/data_scaled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pinned_scaled = pd.get_dummies(df_pinned_scaled, columns=['genre1'], drop_first=True)\n",
    "df_scaled = pd.get_dummies(df_scaled, columns=['genre1'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevars = ['sentiment_prob_pos', 'sentiment_prob_neg', 'lexdiv_cttr', 'num_punct',\n",
    "           'num_sentences', 'SMOG_readability', 'second_person', 'user_follower',\n",
    "           'cosine_1', 'hours_since_article', 'votes_pos_mean', 'votes_neg_mean',\n",
    "           'article_comments', 'is_root_comment', 'level_in_tree'] + [x for x in df_scaled.columns if 'genre1' in x]\n",
    "pinned_var = ['pinned_f']\n",
    "engagevars = ['is_leaf_comment', 'size_of_tree', 'height_of_tree', 'all_replies']\n",
    "votes_vars = ['votes_neg_log', 'votes_pos_log']\n",
    "\n",
    "pinned_varset = [prevars, prevars+engagevars, prevars+engagevars+votes_vars]\n",
    "votes_pos_varset = [prevars, prevars+pinned_var, prevars+pinned_var+engagevars,\n",
    "                    prevars+pinned_var+engagevars+votes_vars[:1]]\n",
    "votes_neg_varset = [prevars, prevars+pinned_var, prevars+pinned_var+engagevars,\n",
    "                    prevars+pinned_var+engagevars+votes_vars[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[pinned_varset[2]], df_scaled['pinned_f'], test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pickle models\n",
    "with open('models/xgb_pinned_models.pkl', 'rb') as f:\n",
    "    models = pickle.load(f)\n",
    "print(len(models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = []\n",
    "\n",
    "for exvars in pinned_varset[len(models):]:\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                        df_scaled['pinned_f'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # cross validated random parameter search\n",
    "    xgb_model = xgb.XGBClassifier()\n",
    "    params = {\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "    rs = RandomizedSearchCV(xgb_model, params, n_iter=20, n_jobs=-1, cv=5, verbose=10)\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    # do grid search around best parameters from random search\n",
    "    best_params = rs.best_params_\n",
    "    print(f\"Best parameters: {best_params}\")\n",
    "    del rs\n",
    "    params = {\n",
    "        'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "        'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "        'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "    gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    # save model\n",
    "    models.append(gs)\n",
    "\n",
    "    # evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exvars = pinned_varset[2]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                    df_scaled['pinned_f'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# cross validated random parameter search\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "\n",
    "best_params = {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.2}\n",
    "\n",
    "params = {\n",
    "    'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "    'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "    'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "# save model\n",
    "models.append(gs)\n",
    "\n",
    "# evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pickle models\n",
    "with open('models/xgb_pinned_models.pkl', 'wb') as f:\n",
    "    pickle.dump(models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup RMSLE loss\n",
    "\n",
    "def gradient(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the gradient squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return (np.log1p(predt) - np.log1p(y)) / (predt + 1)\n",
    "\n",
    "def hessian(predt: np.ndarray, dtrain: xgb.DMatrix) -> np.ndarray:\n",
    "    '''Compute the hessian for squared log error.'''\n",
    "    y = dtrain.get_label()\n",
    "    return ((-np.log1p(predt) + np.log1p(y) + 1) /\n",
    "            np.power(predt + 1, 2))\n",
    "\n",
    "def squared_log(predt: np.ndarray,\n",
    "                dtrain: xgb.DMatrix) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    '''Squared Log Error objective. A simplified version for RMSLE used as\n",
    "    objective function.\n",
    "    '''\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    grad = gradient(predt, dtrain)\n",
    "    hess = hessian(predt, dtrain)\n",
    "    return grad, hess\n",
    "\n",
    "def rmsle(predt: np.ndarray, dtrain: xgb.DMatrix) -> Tuple[str, float]:\n",
    "    ''' Root mean squared log error metric.'''\n",
    "    y = dtrain.get_label()\n",
    "    predt[predt < -1] = -1 + 1e-6\n",
    "    elements = np.power(np.log1p(y) - np.log1p(predt), 2)\n",
    "    return 'PyRMSLE', float(np.sqrt(np.sum(elements) / len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[votes_pos_varset[-1]], df_scaled['votes_pos'], test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "print(f\"Votes Pos RMSLE: {err}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[votes_neg_varset[-1]], df_scaled['votes_neg'], test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "print(f\"Votes Neg RMSLE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgb_votes_pos_models.pkl', 'rb') as f:\n",
    "    votes_pos_models = pickle.load(f)\n",
    "print(len(votes_pos_models))\n",
    "# votes_pos_models = []\n",
    "\n",
    "for exvars in votes_pos_varset[len(votes_pos_models):]:\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                        df_scaled['votes_pos'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # cross validated random parameter search\n",
    "    xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "    params = {\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "    rs = RandomizedSearchCV(xgb_model, params, n_iter=20, n_jobs=-1, cv=5, verbose=10)\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    # do grid search around best parameters from random search\n",
    "    best_params = rs.best_params_\n",
    "    print('Best parameters:', best_params)\n",
    "    del rs\n",
    "    params = {\n",
    "        'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "        'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "        'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "    gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    # save model\n",
    "    votes_pos_models.append(gs)\n",
    "\n",
    "    # evaluate\n",
    "    err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "    print(f\"Votes Pos RMSLE: {err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgb_votes_pos_models.pkl', 'rb') as f:\n",
    "    votes_pos_models = pickle.load(f)\n",
    "print(len(votes_pos_models))\n",
    "    \n",
    "exvars = votes_pos_varset[len(votes_pos_models)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                    df_scaled['votes_pos'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# cross validated random parameter search\n",
    "xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "\n",
    "best_params = {'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.05}\n",
    "\n",
    "params = {\n",
    "    'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "    'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "    'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "# save model\n",
    "votes_pos_models.append(gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle models\n",
    "with open('models/xgb_votes_pos_models.pkl', 'wb') as f:\n",
    "    pickle.dump(votes_pos_models, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgb_votes_neg_models.pkl', 'rb') as f:\n",
    "    votes_neg_models = pickle.load(f)\n",
    "\n",
    "# votes_neg_models = []\n",
    "\n",
    "print(len(votes_neg_models))\n",
    "\n",
    "\n",
    "for exvars in votes_neg_varset[len(votes_neg_models):]:\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                        df_scaled['votes_neg'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # cross validated random parameter search\n",
    "    xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "    params = {\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n",
    "    rs = RandomizedSearchCV(xgb_model, params, n_iter=20, n_jobs=-1, cv=5, verbose=10)\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    # do grid search around best parameters from random search\n",
    "    best_params = rs.best_params_\n",
    "    print(best_params)\n",
    "    params = {\n",
    "        'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "        'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "        'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "    del rs\n",
    "    gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "    gs.fit(X_train, y_train)\n",
    "\n",
    "    # predict\n",
    "    y_pred = gs.predict(X_test)\n",
    "\n",
    "    # save model\n",
    "    votes_neg_models.append(gs)\n",
    "\n",
    "    # evaluate\n",
    "    err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "    print(f\"Votes Neg RMSLE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('models/xgb_votes_neg_models.pkl', 'rb') as f:\n",
    "    votes_neg_models = pickle.load(f)\n",
    "print(len(votes_neg_models))\n",
    "    \n",
    "exvars = votes_neg_varset[len(votes_neg_models)]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                    df_scaled['votes_neg'],\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# cross validated random parameter search\n",
    "xgb_model = xgb.XGBRegressor(obj=squared_log, custom_metric=rmsle, eval_metric=rmsle, disable_default_eval_metric=1)\n",
    "\n",
    "# do grid search around best parameters from random search\n",
    "best_params = {'n_estimators': 1000, 'max_depth': 5, 'learning_rate': 0.05}\n",
    "print(best_params)\n",
    "params = {\n",
    "    'max_depth': [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\n",
    "    'learning_rate': [best_params['learning_rate']*0.8, best_params['learning_rate'], best_params['learning_rate']*1.2],\n",
    "    'n_estimators': [best_params['n_estimators']-100, best_params['n_estimators'], best_params['n_estimators']+100]}\n",
    "gs = GridSearchCV(xgb_model, params, n_jobs=-1, cv=5, verbose=10)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = gs.predict(X_test)\n",
    "\n",
    "# save model\n",
    "votes_neg_models.append(gs)\n",
    "\n",
    "# evaluate\n",
    "err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "print(f\"Votes Neg RMSLE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle models\n",
    "with open('models/xgb_votes_neg_models.pkl', 'wb') as f:\n",
    "    pickle.dump(votes_neg_models, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output predictions for final models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('data/comment_data_general_091022_untilarticle5874_final_redacted.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgb_pinned_models.pkl', 'rb') as f:\n",
    "    models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -1\n",
    "exvars = pinned_varset[n]\n",
    "model = models[n]\n",
    "pred_pinned_xgb = pd.DataFrame({'article': full_data['article'],\n",
    "                                'comment_id': full_data['comment_id'],\n",
    "                                'pred_pinned': model.predict_proba(df_scaled[exvars])[:,1]}) \n",
    "pred_pinned_xgb.to_csv('model_output/xgbs/pred_pinned_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled[pinned_varset[2]], df_scaled['pinned_f'], test_size=0.2, random_state=42)\n",
    "\n",
    "xgb_model = models[-1]\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/xgb_votes_pos_models.pkl', 'rb') as f:\n",
    "    votes_pos_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, model in enumerate(votes_pos_models):\n",
    "    exvars = votes_pos_varset[n]\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                        df_scaled['votes_pos'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "    print(f\"Votes Pos RMSLE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -1\n",
    "exvars = votes_pos_varset[n]\n",
    "model = votes_pos_models[n]\n",
    "pred_votes_pos_xgb = pd.DataFrame({'article': full_data['article'],\n",
    "                                    'comment_id': full_data['comment_id'],\n",
    "                                    'pred_votes_pos': model.predict(df_scaled[exvars])}) \n",
    "pred_votes_pos_xgb.to_csv('model_output/xgbs/pred_votes_pos_xgb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_output/xgbs/xgb_votes_neg_models.pkl', 'rb') as f:\n",
    "    votes_neg_models = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, model in enumerate(votes_neg_models):\n",
    "    exvars = votes_neg_varset[n]\n",
    "\n",
    "    # train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_scaled[exvars],\n",
    "                                                        df_scaled['votes_neg'],\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "\n",
    "    # predict\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # evaluate\n",
    "    err = root_mean_squared_log_error(y_test, np.clip(y_pred, 0, None))\n",
    "    print(f\"Votes Neg RMSLE: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = -1\n",
    "exvars = votes_neg_varset[n]\n",
    "model = votes_neg_models[n]\n",
    "pred_votes_neg_xgb = pd.DataFrame({'article': full_data['article'],\n",
    "                                    'comment_id': full_data['comment_id'],\n",
    "                                    'pred_votes_neg': model.predict(df_scaled[exvars])})\n",
    "pred_votes_neg_xgb.to_csv('model_output/xgbs/pred_votes_neg_xgb.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
